{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3500d767-65b5-4410-a405-424cae87908f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cfe1ed2-cd78-4dd9-af0d-3d1b596d47d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,num_classes=200):\n",
    "        super(Model,self).__init__()\n",
    "      \n",
    "        self.first_layer=nn.Conv2d(in_channels=3,out_channels=96,kernel_size=11,stride=4,padding=2)\n",
    "        self.second_layer=nn.Conv2d(in_channels=96,out_channels=256,kernel_size=5,padding=2)\n",
    "        self.third_layer=nn.Conv2d(in_channels=256,out_channels=384,kernel_size=3,padding=1)\n",
    "        self.forth_layer=nn.Conv2d(in_channels=384,out_channels=384,kernel_size=3,padding=1)\n",
    "        self.fifth_layer=nn.Conv2d(in_channels=384,out_channels=256,kernel_size=3,padding=1)\n",
    "        self.lrn=nn.LocalResponseNorm(size=5,alpha=1e-4, beta=0.75)\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3, stride=2) #z=3,s=2\n",
    "        \n",
    "        self.fc_shape=None\n",
    "        print(\"Fitting dummy data to get the shape of flattening layer\")\n",
    "        if self.fc_shape==None:\n",
    "            dummy_x=torch.randn(1,3,224,224)\n",
    "            self.get_flatten_size(dummy_x)\n",
    "        \n",
    "        self.fc1=nn.Linear(in_features=self.fc_shape,out_features=4096)\n",
    "        self.fc2=nn.Linear(in_features=4096,out_features=4096)\n",
    "\n",
    "        self.output_layer=nn.Linear(4096,num_classes)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "   \n",
    "    \n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    # Biases=1 for conv2/4/5 and FC hidden layers (fc1/fc2)\n",
    "                    if m in [self.second_layer, self.forth_layer, self.fifth_layer, self.fc1, self.fc2]:\n",
    "                        nn.init.constant_(m.bias, 1)\n",
    "                    else:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def get_flatten_size(self,x):\n",
    "        x=F.relu(self.first_layer(x))\n",
    "        x=self.lrn(x)\n",
    "        x=self.maxpool(x)\n",
    "\n",
    "        x=F.relu(self.second_layer(x))\n",
    "        x=self.lrn(x)\n",
    "        x=self.maxpool(x)\n",
    "\n",
    "        x=F.relu(self.third_layer(x))\n",
    "        x=F.relu(self.forth_layer(x))\n",
    "        x=F.relu(self.fifth_layer(x))\n",
    "        x=self.maxpool(x)\n",
    "\n",
    "\n",
    "        self.fc_shape = x.view(x.size(0), -1).size(1)\n",
    "        print(f\"Ran over the dummy data to get the shape for first flattening layer as : {self.fc_shape}\")\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.first_layer(x))\n",
    "        x=self.lrn(x)\n",
    "        x=self.maxpool(x)\n",
    "        x=F.relu(self.second_layer(x))\n",
    "        x=self.lrn(x)\n",
    "        x=self.maxpool(x)\n",
    "        x=F.relu(self.third_layer(x))\n",
    "        x=F.relu(self.forth_layer(x))\n",
    "        x=F.relu(self.fifth_layer(x))\n",
    "        x=self.maxpool(x)\n",
    "        x=x.view(-1,self.fc_shape)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        X=F.dropout(x,0.5)\n",
    "        x=F.relu(self.fc2(x))\n",
    "        X=F.dropout(x,0.5)\n",
    "        x=F.log_softmax(self.output_layer(x),dim=1)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11c9346-936f-472c-b3d0-cf979d2a94fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting dummy data to get the shape of flattening layer\n",
      "Ran over the dummy data to get the shape for first flattening layer as : 9216\n",
      "Model(\n",
      "  (first_layer): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "  (second_layer): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (third_layer): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (forth_layer): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fifth_layer): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (lrn): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1.0)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (output_layer): Linear(in_features=4096, out_features=200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a840faff-de48-43da-ba23-95b9d59500f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"zh-plus/tiny-imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb94820d-a6cb-474c-8f8a-017ecc9973e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = [item['image'] for item in ds['train']]\n",
    "\n",
    "\n",
    "def compute_pca(dataset):\n",
    "    pixels = []  \n",
    "    for img in dataset:\n",
    "        img = img.convert('RGB')  \n",
    "        img_tensor = transforms.ToTensor()(img)  \n",
    "        rearranged = img_tensor.permute(1, 2, 0)  \n",
    "        flattened = rearranged.reshape(-1, 3).numpy()  \n",
    "        pixels.append(flattened)\n",
    "    all_pixels = np.vstack(pixels)\n",
    "    cov = np.cov(all_pixels.T)  \n",
    "    eigval, eigvec = np.linalg.eigh(cov)  \n",
    "    return torch.from_numpy(eigval[::-1].copy()).float(), torch.from_numpy(eigvec[:, ::-1].copy()).float()  \n",
    "\n",
    "eigval, eigvec = compute_pca(images)\n",
    "\n",
    "\n",
    "\n",
    "class PCAColorAugmentation(object):\n",
    "    def __init__(self, eigval, eigvec, alphastd=0.1):\n",
    "        self.eigval = eigval\n",
    "        self.eigvec = eigvec\n",
    "        self.alphastd = alphastd\n",
    "\n",
    "    def __call__(self, img_tensor):\n",
    "        alpha = torch.normal(mean=0.0, std=self.alphastd, size=(3,))\n",
    "        rgb_perturbation = torch.matmul(self.eigvec, alpha * self.eigval)  \n",
    "        \n",
    "        for i in range(3):\n",
    "            img_tensor[i] += rgb_perturbation[i]\n",
    "        \n",
    "        return img_tensor.clamp(0, 1)  \n",
    "\n",
    "\n",
    "class HuggingFaceToPyTorchDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        image = item['image'].convert('RGB')  \n",
    "        label = item['label']  \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    \n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert('RGB')), \n",
    "    transforms.Resize(256),  \n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  \n",
    "    PCAColorAugmentation(eigval, eigvec, alphastd=0.1),  \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert('RGB')),  \n",
    "    transforms.Resize(256),  \n",
    "    transforms.TenCrop(224),  \n",
    "    transforms.Lambda(lambda crops: torch.stack([\n",
    "        transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])(crop) for crop in crops\n",
    "    ]))  \n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = HuggingFaceToPyTorchDataset(ds['train'], transform=train_transforms)\n",
    "\n",
    "valid_dataset = HuggingFaceToPyTorchDataset(ds['valid'], transform=test_transforms)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb954e-ca6a-4e4c-95dd-2dd2ef09d6c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f124c8e2-57da-41ae-b655-e4d7c0095050",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting dummy data to get the shape of flattening layer\n",
      "Ran over the dummy data to get the shape for first flattening layer as : 9216\n",
      "Epoch 001 | step 0050 | loss 5.5930\n",
      "Epoch 001 | step 0100 | loss 5.2998\n",
      "Epoch 001 | step 0150 | loss 5.2983\n",
      "Epoch 001 | step 0200 | loss 5.2984\n",
      "Epoch 001 | step 0250 | loss 5.2986\n",
      "Epoch 001 | step 0300 | loss 5.2986\n",
      "Epoch 001 | step 0350 | loss 5.2987\n",
      "Epoch 001 | step 0400 | loss 5.2986\n",
      "Epoch 001 | step 0450 | loss 5.2987\n",
      "Epoch 001 | step 0500 | loss 5.2986\n",
      "Epoch 001 | step 0550 | loss 5.2990\n",
      "Epoch 001 | step 0600 | loss 5.2988\n",
      "Epoch 001 | step 0650 | loss 5.2987\n",
      "Epoch 001 | step 0700 | loss 5.2988\n",
      "Epoch 001 | step 0750 | loss 5.2989\n",
      "Epoch 001 | val-loss -0.0050 | val-acc 0.50%\n",
      "Checkpoint saved with val-acc 0.50%\n",
      "Epoch 002 | step 0050 | loss 5.2983\n",
      "Epoch 002 | step 0100 | loss 5.2983\n",
      "Epoch 002 | step 0150 | loss 5.2985\n",
      "Epoch 002 | step 0200 | loss 5.2986\n",
      "Epoch 002 | step 0250 | loss 5.2986\n",
      "Epoch 002 | step 0300 | loss 5.2986\n",
      "Epoch 002 | step 0350 | loss 5.2987\n",
      "Epoch 002 | step 0400 | loss 5.2987\n",
      "Epoch 002 | step 0450 | loss 5.2988\n",
      "Epoch 002 | step 0500 | loss 5.2987\n",
      "Epoch 002 | step 0550 | loss 5.2988\n",
      "Epoch 002 | step 0600 | loss 5.2992\n",
      "Epoch 002 | step 0650 | loss 5.2990\n",
      "Epoch 002 | step 0700 | loss 5.2988\n",
      "Epoch 002 | step 0750 | loss 5.2989\n",
      "Epoch 002 | val-loss -0.0050 | val-acc 0.50%\n",
      "Epoch 003 | step 0050 | loss 5.2983\n",
      "Epoch 003 | step 0100 | loss 5.2984\n",
      "Epoch 003 | step 0150 | loss 5.2984\n",
      "Epoch 003 | step 0200 | loss 5.2985\n",
      "Epoch 003 | step 0250 | loss 5.2985\n",
      "Epoch 003 | step 0300 | loss 5.2986\n",
      "Epoch 003 | step 0350 | loss 5.2987\n",
      "Epoch 003 | step 0400 | loss 5.2985\n",
      "Epoch 003 | step 0450 | loss 5.2989\n",
      "Epoch 003 | step 0500 | loss 5.2987\n",
      "Epoch 003 | step 0550 | loss 5.2987\n",
      "Epoch 003 | step 0600 | loss 5.2990\n",
      "Epoch 003 | step 0650 | loss 5.2988\n",
      "Epoch 003 | step 0700 | loss 5.2989\n",
      "Epoch 003 | step 0750 | loss 5.2993\n",
      "Epoch 003 | val-loss -0.0050 | val-acc 0.50%\n",
      "Epoch 004 | step 0050 | loss 5.2983\n",
      "Epoch 004 | step 0100 | loss 5.2984\n",
      "Epoch 004 | step 0150 | loss 5.2985\n",
      "Epoch 004 | step 0200 | loss 5.2984\n",
      "Epoch 004 | step 0250 | loss 5.2987\n",
      "Epoch 004 | step 0300 | loss 5.2987\n",
      "Epoch 004 | step 0350 | loss 5.2987\n",
      "Epoch 004 | step 0400 | loss 5.2987\n",
      "Epoch 004 | step 0450 | loss 5.2988\n",
      "Epoch 004 | step 0500 | loss 5.2988\n",
      "Epoch 004 | step 0550 | loss 5.2990\n",
      "Epoch 004 | step 0600 | loss 5.2988\n",
      "Epoch 004 | step 0650 | loss 5.2988\n",
      "Epoch 004 | step 0700 | loss 5.2988\n",
      "Epoch 004 | step 0750 | loss 5.2990\n",
      "Epoch 004 | val-loss -0.0050 | val-acc 0.50%\n",
      "Epoch 005 | step 0050 | loss 5.2983\n",
      "Epoch 005 | step 0100 | loss 5.2984\n",
      "Epoch 005 | step 0150 | loss 5.2985\n",
      "Epoch 005 | step 0200 | loss 5.2985\n",
      "Epoch 005 | step 0250 | loss 5.2986\n",
      "Epoch 005 | step 0300 | loss 5.2988\n",
      "Epoch 005 | step 0350 | loss 5.2986\n",
      "Epoch 005 | step 0400 | loss 5.2987\n",
      "Epoch 005 | step 0450 | loss 5.2989\n",
      "Epoch 005 | step 0500 | loss 5.2988\n",
      "Epoch 005 | step 0550 | loss 5.2988\n",
      "Epoch 005 | step 0600 | loss 5.2990\n",
      "Epoch 005 | step 0650 | loss 5.2992\n",
      "Epoch 005 | step 0700 | loss 5.2987\n",
      "Epoch 005 | step 0750 | loss 5.2990\n",
      "Epoch 005 | val-loss -0.0050 | val-acc 0.50%\n",
      "Epoch 006 | step 0050 | loss 5.2985\n",
      "Epoch 006 | step 0100 | loss 5.2985\n",
      "Epoch 006 | step 0150 | loss 5.2984\n",
      "Epoch 006 | step 0200 | loss 5.2986\n",
      "Epoch 006 | step 0250 | loss 5.2984\n",
      "Epoch 006 | step 0300 | loss 5.2986\n",
      "Epoch 006 | step 0350 | loss 5.2986\n",
      "Epoch 006 | step 0400 | loss 5.2988\n",
      "Epoch 006 | step 0450 | loss 5.2986\n",
      "Epoch 006 | step 0500 | loss 5.2987\n",
      "Epoch 006 | step 0550 | loss 5.2988\n",
      "Epoch 006 | step 0600 | loss 5.2989\n",
      "Epoch 006 | step 0650 | loss 5.2988\n",
      "Epoch 006 | step 0700 | loss 5.2990\n",
      "Epoch 006 | step 0750 | loss 5.2990\n",
      "Epoch 006 | val-loss -0.0050 | val-acc 0.50%\n",
      "Epoch 007 | step 0050 | loss 5.2984\n",
      "Epoch 007 | step 0100 | loss 5.2985\n",
      "Epoch 007 | step 0150 | loss 5.2984\n",
      "Epoch 007 | step 0200 | loss 5.2984\n",
      "Epoch 007 | step 0250 | loss 5.2986\n",
      "Epoch 007 | step 0300 | loss 5.2986\n",
      "Epoch 007 | step 0350 | loss 5.2986\n",
      "Epoch 007 | step 0400 | loss 5.2986\n",
      "Epoch 007 | step 0450 | loss 5.2988\n",
      "Epoch 007 | step 0500 | loss 5.2990\n",
      "Epoch 007 | step 0550 | loss 5.2992\n",
      "Epoch 007 | step 0600 | loss 5.2989\n",
      "Epoch 007 | step 0650 | loss 5.2988\n",
      "Epoch 007 | step 0700 | loss 5.2989\n",
      "Epoch 007 | step 0750 | loss 5.2990\n",
      "Epoch 007 | val-loss -0.0050 | val-acc 0.50%\n",
      "Epoch 008 | step 0050 | loss 5.2984\n",
      "Epoch 008 | step 0100 | loss 5.2985\n",
      "Epoch 008 | step 0150 | loss 5.2985\n",
      "Epoch 008 | step 0200 | loss 5.2985\n",
      "Epoch 008 | step 0250 | loss 5.2985\n",
      "Epoch 008 | step 0300 | loss 5.2986\n",
      "Epoch 008 | step 0350 | loss 5.2985\n",
      "Epoch 008 | step 0400 | loss 5.2987\n",
      "Epoch 008 | step 0450 | loss 5.2989\n",
      "Epoch 008 | step 0500 | loss 5.2990\n",
      "Epoch 008 | step 0550 | loss 5.2992\n",
      "Epoch 008 | step 0600 | loss 5.2989\n",
      "Epoch 008 | step 0650 | loss 5.2987\n",
      "Epoch 008 | step 0700 | loss 5.2990\n",
      "Epoch 008 | step 0750 | loss 5.2990\n",
      "Epoch 008 | val-loss -0.0050 | val-acc 0.50%\n",
      "Epoch 009 | step 0050 | loss 5.2984\n",
      "Epoch 009 | step 0100 | loss 5.2984\n",
      "Epoch 009 | step 0150 | loss 5.2984\n",
      "Epoch 009 | step 0200 | loss 5.2985\n",
      "Epoch 009 | step 0250 | loss 5.2984\n",
      "Epoch 009 | step 0300 | loss 5.2987\n",
      "Epoch 009 | step 0350 | loss 5.2987\n",
      "Epoch 009 | step 0400 | loss 5.2988\n",
      "Epoch 009 | step 0450 | loss 5.2989\n",
      "Epoch 009 | step 0500 | loss 5.2988\n",
      "Epoch 009 | step 0550 | loss 5.2988\n",
      "Epoch 009 | step 0600 | loss 5.2988\n",
      "Epoch 009 | step 0650 | loss 5.2989\n",
      "Epoch 009 | step 0700 | loss 5.2989\n",
      "Epoch 009 | step 0750 | loss 5.2989\n",
      "Epoch 009 | val-loss -0.0050 | val-acc 0.50%\n",
      "Epoch 010 | step 0050 | loss 5.2983\n",
      "Epoch 010 | step 0100 | loss 5.2984\n",
      "Epoch 010 | step 0150 | loss 5.2983\n",
      "Epoch 010 | step 0200 | loss 5.2987\n",
      "Epoch 010 | step 0250 | loss 5.2986\n",
      "Epoch 010 | step 0300 | loss 5.2985\n",
      "Epoch 010 | step 0350 | loss 5.2990\n",
      "Epoch 010 | step 0400 | loss 5.2988\n",
      "Epoch 010 | step 0450 | loss 5.2988\n",
      "Epoch 010 | step 0500 | loss 5.2987\n",
      "Epoch 010 | step 0550 | loss 5.2987\n",
      "Epoch 010 | step 0600 | loss 5.2988\n",
      "Epoch 010 | step 0650 | loss 5.2987\n",
      "Epoch 010 | step 0700 | loss 5.2988\n",
      "Epoch 010 | step 0750 | loss 5.2991\n",
      "Epoch 010 | val-loss -0.0050 | val-acc 0.50%\n",
      "Final best val-loss: -0.0050 | val-acc: 0.50%\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False, num_workers=4) \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model(num_classes=200).to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)  \n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion=nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)  \n",
    "            batch_size = images.size(0)\n",
    "            images = images.view(batch_size * 10, 3, 224, 224)\n",
    "            outputs = model(images)  \n",
    "            outputs = outputs.view(batch_size, 10, -1)\n",
    "            probs = F.softmax(outputs, dim=2)  \n",
    "            avg_probs = probs.mean(dim=1)  \n",
    "            loss = criterion(avg_probs, labels) \n",
    "            val_loss += loss.item() * batch_size  \n",
    "            predicted = avg_probs.argmax(dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss /= total  \n",
    "    val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc\n",
    "\n",
    "epochs = 10\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch:03d} | step {i+1:04d} | loss {running_loss/50:.4f}\")\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    val_loss, val_acc = evaluate(model, valid_loader)\n",
    "    print(f\"Epoch {epoch:03d} | val-loss {val_loss:.4f} | val-acc {val_acc:.2f}%\")\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"Checkpoint saved with val-acc {val_acc:.2f}%\")\n",
    "    \n",
    "    if optimizer.param_groups[0]['lr'] <= 1e-5:\n",
    "        print(\"Learning-rate floor reached â€” finishing training early.\")\n",
    "        break\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "final_val_loss, final_val_acc = evaluate(model, valid_loader)\n",
    "print(f\"Final best val-loss: {final_val_loss:.4f} | val-acc: {final_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d9b284-8f26-43a3-a94e-bb7d998e6c25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c5482f-d0c7-4f59-a6fa-5261cfa78fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a4a0d1-037d-4cb1-8f97-ca9d68173606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775fb001-a025-4150-8f0c-6b0ae42fa96b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e86a68f-ea2d-4de6-b292-281edaccaee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab507a-3f5a-49a0-860f-ddb3005a6251",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c408f24b-ee5b-4c80-ad89-f493845382f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556cd4c8-e34a-4c84-ae22-447d0a70a7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
